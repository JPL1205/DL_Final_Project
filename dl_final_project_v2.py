# -*- coding: utf-8 -*-
"""DL_Final_Project-v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tf2MlCHuyAWlaHvWTSDgCvcFJLIvTac4
"""

from google.colab import drive
drive.mount('/content/drive')

# import pandas as pd

# # Define the path to your dataset in Google Drive
# dataset_path = '/content/drive/MyDrive/DL_Project/gptJ_subject_check_full.csv'

# # Load the dataset into a pandas DataFrame
# try:
#     df = pd.read_csv(dataset_path)
#     print(f"Successfully loaded dataset from: {dataset_path}")
#     display(df.head())
# except FileNotFoundError:
#     print(f"Error: The file '{dataset_path}' was not found. Please check the path and filename.")
# except Exception as e:
#     print(f"An error occurred while loading the dataset: {e}")

import nltk
nltk.download("punkt")
nltk.download("punkt_tab")  # new NLTK versions split it out

!export HF_DATASETS_TRUST_REMOTE_CODE=1

import torch
print(torch.cuda.is_available(), torch.cuda.get_device_name(0) if torch.cuda.is_available() else "no GPU")

!git clone https://github.com/JPL1205/AlphaEdit.git

!pip install \
  torch==2.6.0 \
  einops==0.8.1 \
  higher==0.2.1 \
  hydra-core==1.3.2 \
  transformers==4.51.3 \
  datasets==2.21.0 \
  matplotlib==3.10.3 \
  "spacy>=3.7,<3.8" \
  scipy==1.15.2 \
  scikit-learn==1.6.1 \
  nltk==3.9.1

!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Commented out IPython magic to ensure Python compatibility.
# %cd AlphaEdit/
!ls

import os
os.environ["HF_DATASETS_TRUST_REMOTE_CODE"] = "1"

"""# Collecting for GPT 2xl"""

!mkdir -p /content/AlphaEdit/data/stats/gpt2-xl/wikipedia_stats
!cp /content/drive/MyDrive/DL_Project/AlphaEdit_cov_stats/gpt2-xl/wikipedia_stats/*.npz \
   /content/AlphaEdit/data/stats/gpt2-xl/wikipedia_stats/

!python3 -m experiments.evaluate \
  --alg_name=ROME \
  --model_name=gpt2-xl \
  --hparams_fname=gpt2-xl.json \
  --ds_name=mcf \
  --dataset_size_limit=2 \
  --num_edits=1 \
  --downstream_eval_steps=0

!python3 -m experiments.evaluate \
    --alg_name=AlphaEdit \
    --model_name=gpt2-xl \
    --hparams_fname=gpt2-xl.json \
    --ds_name=mcf \
    --dataset_size_limit=2 \
    --num_edits=1 \
    --downstream_eval_steps=0

# sync npz for covariance matrix to folder

# import shutil
# from pathlib import Path

# PROJECT_ROOT = Path("/content/AlphaEdit")                 # adjust if your repo lives elsewhere
# STATS_ROOT = PROJECT_ROOT / "data" / "stats"
# DRIVE_DEST = Path("/content/drive/MyDrive/DL_Project/AlphaEdit_cov_stats")

# def backup_cov_stats(model_name="gpt2-xl", mom2_dataset="wikipedia"):
#     src = STATS_ROOT / model_name / f"{mom2_dataset}_stats"
#     if not src.exists():
#         print(f"[backup] No stats at {src}, skipping.")
#         return
#     dst = DRIVE_DEST / model_name / f"{mom2_dataset}_stats"
#     dst.mkdir(parents=True, exist_ok=True)
#     copied = 0
#     for npz_path in src.glob("*.npz"):
#         shutil.copy2(npz_path, dst / npz_path.name)
#         copied += 1
#     print(f"[backup] Copied {copied} files to {dst}")

# backup_cov_stats(model_name="gpt2-xl", mom2_dataset="wikipedia")

# sync run result to folder

# import shutil
# from pathlib import Path

# PROJECT_ROOT = Path("/content/AlphaEdit")                # repo root
# RUN_NAME = "run_006"                                     # change if needed
# RUN_SRC = PROJECT_ROOT / "results" / "AlphaEdit" / RUN_NAME

# DRIVE_ROOT = Path("/content/drive/MyDrive/DL_Project")   # Drive folder you want
# RUN_DST = DRIVE_ROOT / "AlphaEdit_results" / RUN_NAME

# def backup_run():
#     if not RUN_SRC.exists():
#         raise FileNotFoundError(f"Run folder not found: {RUN_SRC}")
#     RUN_DST.mkdir(parents=True, exist_ok=True)
#     shutil.copytree(RUN_SRC, RUN_DST, dirs_exist_ok=True)
#     print(f"Copied {RUN_SRC} → {RUN_DST}")

# backup_run()

"""# Collecting for GPT J 6B"""

!pip uninstall -y torchvision

!python -m experiments.evaluate \
    --alg_name=AlphaEdit \
    --model_name=EleutherAI/gpt-j-6B \
    --hparams_fname=EleutherAI_gpt-j-6B.json \
    --ds_name=mcf \
    --dataset_size_limit=2 \
    --num_edits=1 \
    --downstream_eval_steps=0

# sync npz for covariance matrix to folder

import shutil
from pathlib import Path

PROJECT_ROOT = Path("/content/AlphaEdit")                 # adjust if your repo lives elsewhere
STATS_ROOT = PROJECT_ROOT / "data" / "stats"
DRIVE_DEST = Path("/content/drive/MyDrive/DL_Project/AlphaEdit_cov_stats")

def backup_cov_stats(model_name="gpt-j-6B", mom2_dataset="wikipedia"):
    src = STATS_ROOT / model_name / f"{mom2_dataset}_stats"
    if not src.exists():
        print(f"[backup] No stats at {src}, skipping.")
        return
    dst = DRIVE_DEST / model_name / f"{mom2_dataset}_stats"
    dst.mkdir(parents=True, exist_ok=True)
    copied = 0
    for npz_path in src.glob("*.npz"):
        shutil.copy2(npz_path, dst / npz_path.name)
        copied += 1
    print(f"[backup] Copied {copied} files to {dst}")

backup_cov_stats(model_name="gpt-j-6B", mom2_dataset="wikipedia")

"""## Evaluation"""

import sys
import os

# Add the AlphaEdit project root to sys.path
project_root = '/content/AlphaEdit'
if project_root not in sys.path:
    sys.path.insert(0, project_root)

print(f"Added {project_root} to sys.path")

import os
# summarization of run
!PYTHONPATH=/content/AlphaEdit:$PYTHONPATH python experiments/summarize.py --dir_name=AlphaEdit --runs=run_006



"""# Get Llama model"""

from huggingface_hub import login
login(new_session=False)

from transformers import pipeline

pipe = pipeline("text-generation", model="meta-llama/Meta-Llama-3-8B")

import os
os.environ['HF_TOKEN'] = 'YOUR_TOKEN_HERE'

import os
from huggingface_hub import InferenceClient

prompt = "Can you please let us know more details about your "

if 'pipe' in locals() or 'pipe' in globals():
    outputs = pipe(prompt, max_new_tokens=50, num_return_sequences=1)
    print(outputs[0]['generated_text'])
else:
    print("Error: The 'pipe' text-generation pipeline is not initialized. Please run the cell 'fRsRJL6ZTqJK' first.")

"""## Llama Inference

Logging in just seeds your session with the HF token—huggingface_hub.login() stores the token in ~/.cache/huggingface/token and sets the HUGGING_FACE_HUB_TOKEN env var. When experiments.evaluate calls AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-8B"), the transformers library automatically reads that token to authenticate the download. That’s why you don’t see it referenced inside the evaluation script: the from_pretrained call handles it under the hood. Once you’ve logged in for the session, you can treat meta-llama/... like any public model; no additional wiring between your login cell and AlphaEdit is required.
"""

!pip install bitsandbytes accelerate

!pip install --upgrade transformers tokenizers bitsandbytes accelerate

!mkdir -p /content/AlphaEdit/data/stats/Meta-Llama-3-8B/wikipedia_stats
!cp /content/drive/MyDrive/DL_Project/AlphaEdit_cov_stats/llama3-8b-instruct/wikipedia_stats/*.npz \
      /content/AlphaEdit/data/stats/Meta-Llama-3-8B/wikipedia_stats/

!ls -l data/stats/meta-llama_Meta-Llama-3-8B/wikipedia_stats

from pathlib import Path

root = Path("/content/AlphaEdit/data/stats/Meta-Llama-3-8B/wikipedia_stats")
for layer in range(4, 9):
    fname = root / f"model.layers.{layer}.mlp.down_proj_float32_mom2_100000.npz"
    print(layer, fname.exists(), fname.stat().st_size if fname.exists() else 0)

!python -m experiments.evaluate \
    --alg_name=AlphaEdit \
    --model_name=meta-llama/Meta-Llama-3-8B \
    --hparams_fname=Llama3-8B.json \
    --ds_name=mcf \
    --dataset_size_limit=2 \
    --num_edits=1 \
    --downstream_eval_steps=0

